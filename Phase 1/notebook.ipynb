{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b2UsRU4P9JF"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۲۸ اسفند ۱۳۹۹\n",
    "<br>\n",
    "<br>\n",
    "<font size=4.8>\n",
    "دستیاران آموزشی: نیما جمالی، آرمین سعادت، ایمان غلامی\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "علی بالاپور - xxxxxxxxx\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3YoNVsQR3zO"
   },
   "source": [
    "<p></p>\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "    هدف از فاز اول پروژه، طراحی و پیاده‌سازی سیستم بازیابی اطلاعات برای مجموعه دادگان تعیین شده می‌باشد.<br>\n",
    "    اولین مرحله، پیش‌پردازش مجموعه دادگان است. پس از آن نمایه‌ها با ویژگی‌های خواسته شده پیاده‌سازی می‌شود. در گام بعدی به ذخیره و بازخوانی نمایه‌ها به همراه روش‌های فشرده‌سازی پرداخته می‌شود. پس از آن تکنیک‌های اصلاح پرسمان پیاده‌سازی شده و در نهایت جستجو روی دادگان صورت می‌گیرد. همچنین در بخش آخر با پیاده‌سازی برخی معیارهای ارزیابی، عملکرد سیستم مورد سنجش قرار می‌گیرد.<br><br>\n",
    "     توضیحات مربوط به هر بخش در ادامه آمده است که اهداف، محدودیت‌ها و خواسته‌های آن بخش را مشخص می‌کند.\n",
    "     در هر بخش توابعی مشخص شده است که محتوای آن با کد نوشته شده توسط شما باید پر شود. شما می‌توانید در همین فایل، سیستم بازیابی خود را پیاده‌سازی کنید یا فایل‌های خودتان را ایمپورت کرده و از آن‌ها استفاده کنید. در هر صورت تمامی کدهای لازم را به همراه این نوت‌بوک ارسال کنید. همچنین در نظر داشته باشید که ملاک اصلی نمره‌دهی شما اجرای صحیح توابع مشخص‌شده در این نوت‌بوک می‌باشد. بنابراین از صحت اجرای نوت‌بوک خود اطمینان پیدا کنید.<br><br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. پروژه ۱۱۵ نمره دارد که ۱۵ نمره از آن امتیازی و مربوط به بخش اصلاح پرسمان می‌باشد.\n",
    "    \n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaR5CS_khMQB"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مجموعه دادگان</b>\n",
    "    </h1>\n",
    "    مجموعه دادگان مورد بررسی در این پروژه از سایت kaggle فراهم شده است. این مجموعه شامل اطلاعات ۶۰۰۰ فیلم سینمایی از سال ۱۹۰۴ تا ۲۰۱۷ است. داده‌ها در قالب فایل csv   دارای ستون‌‌های plot، title، id می‌باشد. id یک شناسه یکتا برای هر فیلم است که برای ارزیابی بهتر عملکرد شما به داده‌ها اضافه شده است و در مجموعه دادگان اصلی وجود نداشته است. همانطور که می‌دانید برای پیاده‌سازی نمایه باید به هر داکیومنت یک شناسه اختصاص بدهید. <b>شناسه مربوط به هر داکیومنت باید id ذکر شده برای آن در مجموعه دادگان باشد.</b> هر فیلم از دو بخش title و plot  تشکیل شده است که از این دو بخش در ساخت نمایه و جستجو استفاده می‌شود. plot خلاصه‌ای از طرح داستان فیلم است.<br>\n",
    "    علاوه بر مجموعه دادگان اصلی، تعدای پرسمان در اختیار شما قرار گرفته است. همچنین جواب مطلوب هر یک از این پرسمان‌ها نیز فراهم شده که طبیعتا زیرمجموعه‌ای از مجموعه دادگان است. شما باید از این پرسمان‌ها و نتیجه مورد انتظار هر کدام برای ارزیابی سامانه خود استفاده کنید.\n",
    "    پرسمان‌ها و شناسه فیلم‌های بازیابی شده مورد انتظار از هر پرسمان در فایل validation.json آمده است. توضیحات بیشتر در رابطه با استفاده از این فایل در بخش ارزیابی آمده است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxNntHofmHHH"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیش‌پردازش و آماده‌سازی داده‌ها (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش ابتدا داده‌ها را از فایل بخوانید. برای آماده‌سازی متن می‌توانید از کتاب‌خانه‌های آماده استفاده کنید. یکی از کتاب‌خانه‌های معروف برای این کار <a href=\"https://www.nltk.org/\">NLTK</a> است اما در انتخاب روش پیاده‌سازی این بخش مختارید. برای این بخش باید تابع ()prepare_text را تکمیل کنید. این تابع یک متن انگلیسی ورودی گرفته و توکن‌‌های مربوط به آن‌را در قالب یک لیست خروجی می‌دهد. متن ورودی در عمل تایتل یا طرح داستان هر فیلم است. دقت کنید که لیست خروجی شامل تعدادی توکن است که عملیات case folding، stemming و lemmatization روی آن‌ها اجرا شده است. در ضمن علائم نگارشی نباید به عنوان توکن در نظر گرفته شود. در کد زیر یک نمونه ورودی و خروجی نمایش داده شده است. با توجه به نحوه پیاده‌سازی انواع بازگردانی به ریشه قابل قبول است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "با توجه به این که دیتاست در فرمت csv می‌باشد و همچنین راحتی کار با کتابخانه pandas، این دیتاست را بصورت یک dataFrame را خوانده و آن را در متغیر df ذخیره می‌کنیم. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "برای این قسمت، تابع prepare_text تکمیل شده‌است. برای این کار ابتدا با استفاده از regex واژه‌های جمله را از یکدیگر جدا کرده(طی این فرآیند، علائم نگارشی حذف می‌شوند.) و سپس با استفاده از تابع str.lower عملیات case folding را انجام می‌دهیم. پس از آن با استفاده از  <a href=\"https://www.nltk.org/howto/stem.html\">Porter Stemmer</a> فرآیند Stemming انجام می‌شود. توجه کنید که در این قسمت، تنها عملیات Stemming انجام می‌شود و روی متن هیچ lemmatization ای انجام نمی‌شود. <br>\n",
    "نکته مهم دیگر این است که با توجه به این که کارهای ذکر شده در بالا، هیچ تاثیری بر روی «'» موجود در کلماتی مانند doesn't نداشتند، بالاجبار، عملیات حذف «'» به صورت دستی و با استفاده از تابع replace انجام گرفت. <br>\n",
    "همچنین در ادامه کار، در قسمتی از کد، لازم بود که متن tokenize شده اما بدون stemming داشته باشیم. به همین خاطر یک آرگومان پیش فرض با نام with_stemming نیز به تابع افزوده شد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aa16Puk-if0",
    "outputId": "2e6a2c95-41d5-4146-a27b-5ceba034e291"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edvard', 'wa', 'a', 'runner', 'doesnt', 'he', 'wa', 'alway', 'run']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_text(raw_text, with_stemming = True):  \n",
    "    regexTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    raw_text = raw_text.replace('\\'', \"\")\n",
    "    tokens = regexTokenizer.tokenize(raw_text)   # tokenization\n",
    "    lower_tokens = list(map(str.lower, tokens))  # case folding\n",
    "    if (with_stemming):\n",
    "        tokens = list(map(porter.stem, lower_tokens))  # stemming\n",
    "    return tokens\n",
    "    \n",
    "prepare_text(\"Edvard was a Runner doesn't. He was always running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVhVEO6VARIa"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>شناسایی و حذف stop-words (۵ نمره)</b>\n",
    "    </h1>\n",
    "    این بخش باید توسط خودتان و بدون استفاده از کد آماده پیاده‌سازی شود. ترم‌های موجود در مجموعه دادگان را بر اساس تکرار آن‌ها مرتب کرده و پرتکرارترین آن‌ها را به عنوان stop-words در نظر بگیرید. اینکه چند ترم را به عنوان stop-words در نظر بگیرید به عهده خودتان است.<br>\n",
    "    با فراخوانی تابع ()get_stop_words لیست stop-words به همراه تعداد تکرار آن‌‌ها خروجی داده می‌شود.<br>\n",
    "    ترم‌های به دست آمده از این بخش نباید در نمایه حضور داشته باشند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "برای این قسمت، تابعبرای این قسمت، طبق راهنمایی ذکر شده عمل شد. یعنی term هایی با بیشترین تکرار به عنوان stop word در نظر گرفته شده است. برای حذف کردن این stop word ها از نمایه، صرفا در صورتی که term ای مشاهده شود که در stop word قرار دارد، این term نادیده گرفته می‌شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-0YxOUeBGDY",
    "outputId": "b8e3476a-02af-41dc-b4a3-5c637ffd4118"
   },
   "outputs": [],
   "source": [
    "dictionary_by_frequency = dict()\n",
    "\n",
    "def get_stop_words(expected_number = 15):\n",
    "    global dictionary_by_frequency\n",
    "    \n",
    "    # tokenize titles in dataframe and save their frequencies\n",
    "    for x in df.title:\n",
    "        x = prepare_text(x)\n",
    "        for y in x:\n",
    "            if y in dictionary_by_frequency.keys():\n",
    "                dictionary_by_frequency[y] += 1\n",
    "            else:\n",
    "                dictionary_by_frequency[y] = 1\n",
    "\n",
    "    # concating the plots with each other, then tokenizing it\n",
    "    plots = \"\"\n",
    "    for x in df[\"plot\"]:\n",
    "        plots += x + \" \"\n",
    "    plots_tokens = prepare_text(plots)\n",
    "\n",
    "    # tokenize plots in dataframe and save their frequencies\n",
    "    for x in plots_tokens:\n",
    "        if x in dictionary_by_frequency.keys():\n",
    "            dictionary_by_frequency[x] += 1\n",
    "        else:\n",
    "            dictionary_by_frequency[x] = 1\n",
    "\n",
    "    # sort dictionary by frequency \n",
    "    dictionary_by_frequency = dict(sorted(dictionary_by_frequency.items(), key = lambda item : item[1])[::-1])\n",
    "    \n",
    "    return dict(itertools.islice(dictionary_by_frequency.items(), expected_number))\n",
    "\n",
    "stop_words = get_stop_words(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 147470,\n",
       " 'to': 91733,\n",
       " 'and': 83270,\n",
       " 'a': 68050,\n",
       " 'of': 42862,\n",
       " 'is': 40230,\n",
       " 'in': 38985,\n",
       " 'hi': 34776,\n",
       " 'he': 32226,\n",
       " 'her': 28025,\n",
       " 'that': 26299,\n",
       " 'with': 25064,\n",
       " 'him': 18931,\n",
       " 'by': 17855,\n",
       " 'for': 17608,\n",
       " 'she': 17532,\n",
       " 'as': 15646,\n",
       " 'on': 14962,\n",
       " 'but': 13532,\n",
       " 'they': 13166,\n",
       " 'who': 12735,\n",
       " 'at': 12353,\n",
       " 'from': 11098,\n",
       " 'ha': 10964,\n",
       " 'an': 10857,\n",
       " 'it': 10212,\n",
       " 'when': 9993,\n",
       " 'be': 9604,\n",
       " 'their': 9505,\n",
       " 'are': 9236}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4Z02BzNHD1z"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نمایه‌سازی (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید برای سامانه positional index بسازید. برای هر ترم باید مشخص باشد که آن ترم در تایتل چه فیلم‌هایی و در چه جایگاهی از تایتل هر فیلم قرار گرفته است. همچنین برای هر ترم باید مشخص باشد که آن ترم در طرح داستان چه فیلم‌هایی و در چه جایگاهی از طرح داستان هر فیلم قرار گرفته است.<br>\n",
    "     برای ارزیابی بهتر و عادلانه‌تر به ویژه برای ارزیابی بخش فشرده‌سازی، از استاندارد بیان شده در قطعه کد زیر برای ذخیره posting list هر ترم در RAM استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "برای این قسمت، نمایه‌ها بر اساس فرمت گفته شده، برای هر term ساخته می‌شود و سپس در دیکشنری term_dictionary قرار داده می‌شود.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJy4LiraBm8E",
    "outputId": "a740d94d-1896-4a6a-9a28-68b1cb8d32db"
   },
   "outputs": [],
   "source": [
    "terms = dictionary_by_frequency.keys() - stop_words.keys()\n",
    "term_dictionary = dict()\n",
    "for term in terms:\n",
    "    term_dictionary[term] = [[] for _ in range(2)]\n",
    "        \n",
    "def create_positional_index():\n",
    "    global term_dictionary\n",
    "\n",
    "    for x in range(len(df.title)):    # x is docID\n",
    "        text = df['title'][x]\n",
    "        docID = df['id'][x]\n",
    "        tokenized_text = prepare_text(text)\n",
    "        for y in range(len(tokenized_text)):   # y is position\n",
    "            term = tokenized_text[y]\n",
    "\n",
    "            if term in stop_words:\n",
    "                continue\n",
    "\n",
    "            title_posting_list = term_dictionary[term][0]\n",
    "            if len(title_posting_list) != 0:\n",
    "                if title_posting_list[-1][0] == docID:\n",
    "                    title_posting_list[-1][1].append(y)\n",
    "                else:\n",
    "                    title_posting_list.append([int(docID), [y]])\n",
    "            else:\n",
    "                title_posting_list.append([int(docID), [y]])\n",
    "                \n",
    "    for x in range(len(df[\"plot\"])):    # x is docID\n",
    "        text = df['plot'][x]\n",
    "        docID = df['id'][x]\n",
    "        tokenized_text = prepare_text(text)\n",
    "        for y in range(len(tokenized_text)):   # y is position\n",
    "            term = tokenized_text[y]\n",
    "\n",
    "            if term in stop_words:\n",
    "                continue\n",
    "\n",
    "            plot_posting_list = term_dictionary[term][1]\n",
    "            if len(plot_posting_list) != 0:\n",
    "                if plot_posting_list[-1][0] == docID:\n",
    "                    plot_posting_list[-1][1].append(y)\n",
    "                else:\n",
    "                    plot_posting_list.append([int(docID), [y]])\n",
    "            else:\n",
    "                plot_posting_list.append([int(docID), [y]])\n",
    "\n",
    "    return term_dictionary\n",
    "\n",
    "term_dictionary = create_positional_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_dictionary[\"phantom\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voqMrk4rg1qk"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پویا‌سازی نمایه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    نمایه ایجاد شده باید قابلیت حذف و اضافه تک داکیومنت را داشته باشد.\n",
    "    برای اضافه شدن داکیومنت، به تابع ()add_single_document یک رشته داده می‌شود که اطلاعات مربوط به داکیومنت شامل id و plot و title در آن با کاما جدا شده است. برای حذف داکیومنت نیز id آن به تابع ()remove_single_document داده می‌شود.<br>\n",
    "    تضمین می‌شود که شرط یکتا بودن id داکیومنت‌ها نقض نشود. برای مثال دو داکیومنت با شناسه یکسان به مجموعه اضافه نخواهد شد. البته ممکن است حذف شده و دوباره اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "برای اضافه کردن یک داکیومنت جدید، ابتدا یک سطر جدید با مشخصات داده شده به df اضافه می‌کنیم و سپس دقیقا مانند قسمت ایجاد نمایه برای term ها عمل کرده و به ازای title و plot جدید داده شده، term های آن را جدا کرده (در اصل تابع prepare_text را صدا می‌زنیم.) و term ها را به term_dictionary اضافه می کنیم. در صورتی که term جدید در داخل term_dictionary نبود، در این صورت به ازای این ترم جدید، یک لیست خالی درست کرده و آن را در term_dictionary قرار می دهیم و سپس عملیات را ادامه می‌دهیم. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e1Ej2n3MB6ef"
   },
   "outputs": [],
   "source": [
    "def add_single_documnet(document):\n",
    "    new_document = document.split(',')\n",
    "    new_id = new_document[0]\n",
    "    new_title = new_document[1]\n",
    "    new_plot = new_document[2]\n",
    "\n",
    "    # add document to dataframe\n",
    "    new_row = {\"id\" : new_id, \"title\" : new_title, \"plot\" : new_plot}\n",
    "    df.append(new_row, ignore_index=True)\n",
    "\n",
    "    # add title terms to index\n",
    "    tokenized_new_title = prepare_text(new_title)\n",
    "    for y in range(len(tokenized_new_title)):   # y is position\n",
    "        term = tokenized_new_title[y]\n",
    "\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        if term not in terms:\n",
    "            terms.add(term)\n",
    "            term_dictionary[term] = [[] for _ in range(2)]\n",
    "\n",
    "        title_posting_list = term_dictionary[term][0]\n",
    "        if len(title_posting_list) != 0:\n",
    "            if title_posting_list[-1][0] == int(new_id):\n",
    "                title_posting_list[-1][1].append(y)\n",
    "            else:\n",
    "                title_posting_list.append([int(new_id), [y]])\n",
    "        else:\n",
    "            title_posting_list.append([int(new_id), [y]])\n",
    "\n",
    "    # add plot terms to index   \n",
    "    tokenized_new_plot = prepare_text(new_plot)\n",
    "    for y in range(len(tokenized_new_plot)):   # y is position\n",
    "        term = tokenized_new_plot[y]\n",
    "\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        if term not in terms:\n",
    "            terms.add(term)\n",
    "            term_dictionary[term] = [[] for _ in range(2)]\n",
    "\n",
    "        plot_posting_list = term_dictionary[term][1]\n",
    "        if len(plot_posting_list) != 0:\n",
    "            if plot_posting_list[-1][0] == int(new_id):\n",
    "                plot_posting_list[-1][1].append(y)\n",
    "            else:\n",
    "                plot_posting_list.append([int(new_id), [y]])\n",
    "        else:\n",
    "            plot_posting_list.append([int(new_id), [y]])\n",
    "    return\n",
    "\n",
    "new_document = \"12134,The Adventures of Sherlock Holmes,The picture begins with Moriarty and Holmes verbally sparring on the steps\"\n",
    "add_single_documnet(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[39, [0]],\n",
       "  [854, [3]],\n",
       "  [1138, [0]],\n",
       "  [1224, [0]],\n",
       "  [1225, [0]],\n",
       "  [1226, [0]],\n",
       "  [1392, [0]],\n",
       "  [5597, [0]],\n",
       "  [12134, [3]]],\n",
       " [[762, [209]],\n",
       "  [858, [0]],\n",
       "  [1138, [255]],\n",
       "  [1224, [0]],\n",
       "  [1225, [107, 125]],\n",
       "  [1335, [2]],\n",
       "  [3235, [18]],\n",
       "  [3417, [11, 54]],\n",
       "  [5597, [4]]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_dictionary['sherlock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "برای حذف یک داکیومنت، به صورت brute force عمل شده‌است. یعنی به ازای همه ترم‌های موجود در term_dictionary، شناسه داک مورد نظر حذف می‌شود. همچنین داکیومنت مدنظر از داخل دیتافریم df نیز حذف می‌شود.<br>\n",
    "روش دیگر برای حذف داکیومنت، استفاده از یک لیست جدا و ذخیره شناسه داکیومنت‌هایی که می‌خواهیم حذف کنیم، است. در این روش، در صورتی که در قسمت‌های دیگر با داکیومنتی مواجه شدیم که جزو داکیومنت‌های حذف شده بود، در این صورت این داکیومنت را نادیده می‌گیریم. به دلیل پیچیدگی در پیاده‌سازی این روش، روش اول یا brute force ترجیح داده شد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DsarzVzhDoMi"
   },
   "outputs": [],
   "source": [
    "def remove_signle_document(document_id):\n",
    "    df.drop(index = df[df[\"id\"] == document_id].index, inplace = True)\n",
    "    for term in term_dictionary.items():\n",
    "        L = term[1]\n",
    "\n",
    "        i = 0\n",
    "        length = len(L[0])\n",
    "        while i < length:\n",
    "            if L[0][i][0] == document_id:\n",
    "                del L[0][i]   \n",
    "                length -= 1\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        length = len(L[1])\n",
    "        while i < length:\n",
    "            if L[1][i][0] == document_id:\n",
    "                del L[1][i]   \n",
    "                length -= 1\n",
    "            i += 1\n",
    "    return\n",
    "  \n",
    "remove_signle_document(12134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[39, [0]],\n",
       "  [854, [3]],\n",
       "  [1138, [0]],\n",
       "  [1224, [0]],\n",
       "  [1225, [0]],\n",
       "  [1226, [0]],\n",
       "  [1392, [0]],\n",
       "  [5597, [0]]],\n",
       " [[762, [209]],\n",
       "  [858, [0]],\n",
       "  [1138, [255]],\n",
       "  [1224, [0]],\n",
       "  [1225, [107, 125]],\n",
       "  [1335, [2]],\n",
       "  [3235, [18]],\n",
       "  [3417, [11, 54]],\n",
       "  [5597, [4]]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_dictionary['sherlock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PBbFcmpXFKD"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ذخیره و فشرده‌سازی نمایه (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید توانایی ذخیره کردن نمایه و بارگذاری مجدد آن را به سامانه اضافه کنید. ذخیره‌سازی به ۳ روش صورت می‌گیرد. بدون فشرده‌سازی، فشرده‌سازی از روش gamma-code و فشرده‌سازی از روش variable-byte. روش‌های فشرده‌سازی باید توسط خودتان پیاده‌سازی شود.\n",
    "    برای ذخیره نمایه در فایل نیز از JSON  استفاده کنید.<br>\n",
    "     بخشی از نمره شما در این قسمت به میزان فشرده‌سازی نمایه اختصاص داده شده است. بنابراین پیاده‌سازی بهینه روش‌های فشرده‌سازی مهم است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "ابتدا لازم است که یک عملیات پیش پردازش بر روی دیکشنری انجام دهیم و اندیس‌های postitional را به صورت gap ذخیره کنیم. یعنی به جای نگه داری تمامی postiotion های هر ترم در لیست، صرفا فاصله بین آن‌ها را ذخیره کنیم. سپس در دو قسمت پایین، توابعی نوشته شده است که term_dictionary را به variable byte و gamma code تبدیل می‌کنند. برای کاهش حجم ذخیره سازی، ابتدا به ازای هر دو posting list ذخیره شده به ازای هر ترم، عملیات فشرده سازی انجام می‌شود و سپس یک دنباله با تعداد اعداد زوج که جمله قرار گرفته در جایگاه زوج نشان دهنده docID و جمله بعدی نشاندهنده شکل فشرده شده و decimal برای postional index می‌باشد خواهیم داشت. سپس این دنباله عددی را ذخیره این دو دنباله عددی را به صورت یک رشته ذخیره می‌کنیم و یک dictionary پایتون را ایجاد می‌کنیم. نتایج این نحوه ذخیره‌سازی، در قسمت‌های پایین ذکر شده است. <br>\n",
    "برای درک بهتر نحوه ذخیره‌سازی، مثال زیر تهیه شده است. در قالب فشرده شده، به ازای هر ترم، یک لیست با دو استرینگ خواهیم داشت که در آن یک دنباله عددی با تعداد اعداد زوج، داریم. جمله در جایگاه زوج نشان دهنده docID و جمله بعدی، نشان دهنده شکل decimal برای فرمت variable byte می‌باشد. یعنی ابتدا فشرده سازی به صورت variable byte انجام می‌شود و سپس یک عدد باینری خواهیم داشت. این عدد باینری را به decimal تبدیل کرده و سپس همین عدد را در استرینگ و در جایگاه خود ذخیره می‌کنیم. در نهایت به ازای هر ترم دو استرینگ، یکی برای پلات و دیگری برای title خواهیم داشت. همانند مثال زیر.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sherlock': [[[39, [0]],\n",
       "   [854, [3]],\n",
       "   [1138, [0]],\n",
       "   [1224, [0]],\n",
       "   [1225, [0]],\n",
       "   [1226, [0]],\n",
       "   [1392, [0]],\n",
       "   [5597, [0]]],\n",
       "  [[762, [209]],\n",
       "   [858, [0]],\n",
       "   [1138, [255]],\n",
       "   [1224, [0]],\n",
       "   [1225, [107, 125]],\n",
       "   [1335, [2]],\n",
       "   [3235, [18]],\n",
       "   [3417, [11, 54]],\n",
       "   [5597, [4]]]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = {'sherlock' : term_dictionary['sherlock']}\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sherlock': ['39,128,854,131,1138,128,1224,128,1225,128,1226,128,1392,128,5597,128',\n",
       "  '762,465,858,128,1138,511,1224,128,1225,60413,1335,130,3235,146,3417,35766,5597,132']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'sherlock': ['39,128,854,131,1138,128,1224,128,1225,128,1226,128,1392,128,5597,128',\n",
    "  '762,465,858,128,1138,511,1224,128,1225,60413,1335,130,3235,146,3417,35766,5597,132']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_storing():\n",
    "    term_dictionary_with_positional_gap = dict()\n",
    "\n",
    "    for key, value in term_dictionary.items():\n",
    "        term_dictionary_with_positional_gap[key] = [[] for _ in range(2)]\n",
    "        new_list_1 = term_dictionary_with_positional_gap[key][0]\n",
    "        new_list_2 = term_dictionary_with_positional_gap[key][1]\n",
    "\n",
    "        old_list_1 = value[0]\n",
    "        old_list_2 = value[1]\n",
    "\n",
    "        for dId_index in range(len(old_list_1)):\n",
    "            dId = old_list_1[dId_index][0]            # dId = document ID\n",
    "            L = old_list_1[dId_index][1]              # positinal index of dId\n",
    "            new_poitional_list = []\n",
    "            for i in range(len(L)):\n",
    "                if i == 0:\n",
    "                    new_poitional_list.append(L[i])\n",
    "                else:\n",
    "                    new_poitional_list.append(L[i] - L[i-1])\n",
    "            new_list_1.append([int(dId), new_poitional_list])\n",
    "\n",
    "\n",
    "        for dId_index in range(len(old_list_2)):\n",
    "            dId = old_list_2[dId_index][0]            # dId = document ID\n",
    "            L = old_list_2[dId_index][1]              # positinal index of dId\n",
    "            new_poitional_list = []\n",
    "            for i in range(len(L)):\n",
    "                if i == 0:\n",
    "                    new_poitional_list.append(L[i])\n",
    "                else:\n",
    "                    new_poitional_list.append(L[i] - L[i-1])\n",
    "            new_list_2.append([int(dId), new_poitional_list])\n",
    "    return term_dictionary_with_positional_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Variable Byte Positional index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_binary_seven_divisable(number):\n",
    "    binary = \"\"\n",
    "    if number == 0:\n",
    "        return '0000000'\n",
    "    while number >= 1:\n",
    "        binary += str(number%2)\n",
    "        number //= 2\n",
    "    if len(binary) % 7 != 0:\n",
    "        zeros = '0' * (7 - (len(binary) % 7))\n",
    "        binary = binary + zeros \n",
    "    return binary[::-1]\n",
    "\n",
    "def convert_to_variable_byte(List):    # get a position of terms\n",
    "    variable_byte = \"\"\n",
    "    for i in range(len(List)):\n",
    "        number = List[i]\n",
    "        binary = decimal_to_binary_seven_divisable(number)\n",
    "        s = \"\"\n",
    "        if len(binary) <= 7:\n",
    "            s = '1' + binary\n",
    "            variable_byte += s\n",
    "            continue\n",
    "        length = len(binary)\n",
    "        while length > 7:\n",
    "            s += '0' + binary[:7]\n",
    "            length -= 7\n",
    "            binary = binary[7:]\n",
    "        s += '1' + binary\n",
    "        \n",
    "        variable_byte += s\n",
    "    return int(variable_byte, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_variable_byte(term_dictionary_with_positional_gap):\n",
    "    term_dictionary_with_positional_gap_variable_byte = dict()\n",
    "\n",
    "    for key, value in term_dictionary_with_positional_gap.items():\n",
    "        term_dictionary_with_positional_gap_variable_byte[key] = []\n",
    "\n",
    "        old_list_1 = value[0]\n",
    "        old_list_2 = value[1]\n",
    "\n",
    "        ans1 = ''\n",
    "        for dId_index in range(len(old_list_1)):\n",
    "            dId = old_list_1[dId_index][0]            # dId = document ID\n",
    "            L = old_list_1[dId_index][1]              # positinal index of dId\n",
    "            ans1 += (str(int(dId)) + ',' + str(convert_to_variable_byte(L)))\n",
    "            if dId_index != len(old_list_1) - 1:\n",
    "                ans1 += ','\n",
    "        term_dictionary_with_positional_gap_variable_byte[key].append(ans1)  \n",
    "\n",
    "        ans2 = ''\n",
    "        for dId_index in range(len(old_list_2)):\n",
    "            dId = old_list_2[dId_index][0]            # dId = document ID\n",
    "            L = old_list_2[dId_index][1]              # positinal index of dId\n",
    "            ans2 += (str(int(dId)) + ',' + str(convert_to_variable_byte(L)))\n",
    "            if dId_index != len(old_list_2) - 1:\n",
    "                ans2 += ','\n",
    "        term_dictionary_with_positional_gap_variable_byte[key].append(ans2) \n",
    "    return term_dictionary_with_positional_gap_variable_byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Gamma Code Positional index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_binary(number):\n",
    "    binary = \"\"\n",
    "    if number == 0:\n",
    "        return '0'\n",
    "    while number >= 1:\n",
    "        binary += str(number%2)\n",
    "        number //= 2 \n",
    "    return binary[::-1]\n",
    "\n",
    "def build_length(number):\n",
    "    length = len(number)\n",
    "    length = '1' * length + '0'\n",
    "    return length\n",
    "    \n",
    "def build_offset(number):\n",
    "    offset = decimal_to_binary(number)\n",
    "    offset = offset[1:]\n",
    "    return offset\n",
    "\n",
    "def gamma_code(List):\n",
    "    res = \"\"\n",
    "    for i in List:\n",
    "        offset = build_offset(i)\n",
    "        length = build_length(offset)\n",
    "        res += length + offset\n",
    "    if res[0] == '0' and List[0] == 1:\n",
    "        return float(int(res, 2))\n",
    "    else:\n",
    "        return int(res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gamma_code(term_dictionary_with_positional_gap):\n",
    "    term_dictionary_with_positional_gap_gamma_code = dict()\n",
    "\n",
    "    for key, value in term_dictionary_with_positional_gap.items():\n",
    "        term_dictionary_with_positional_gap_gamma_code[key] = []\n",
    "\n",
    "        old_list_1 = value[0]\n",
    "        old_list_2 = value[1]\n",
    "\n",
    "        ans1 = ''\n",
    "        for dId_index in range(len(old_list_1)):\n",
    "            dId = old_list_1[dId_index][0]            # dId = document ID\n",
    "            L = old_list_1[dId_index][1]              # positinal index of dId\n",
    "            ans1 += (str(int(dId)) + ',' + str(gamma_code(L)))\n",
    "            if dId_index != len(old_list_1) - 1:\n",
    "                ans1 += ','\n",
    "        term_dictionary_with_positional_gap_gamma_code[key].append(ans1)\n",
    "\n",
    "        ans2 = ''  \n",
    "        for dId_index in range(len(old_list_2)):\n",
    "            dId = old_list_2[dId_index][0]            # dId = document ID\n",
    "            L = old_list_2[dId_index][1]              # positinal index of dId\n",
    "            ans2 += (str(int(dId)) + ',' + str(gamma_code(L)))\n",
    "            if dId_index != len(old_list_2) - 1:\n",
    "                ans2 += ','\n",
    "        term_dictionary_with_positional_gap_gamma_code[key].append(ans2)\n",
    "    return term_dictionary_with_positional_gap_gamma_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYN9I4_BD178",
    "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no compression size :  17791041  bytes\n",
      "gamma code size :  13546253  bytes\n",
      "variable byte size :  11617938  bytes\n"
     ]
    }
   ],
   "source": [
    "def store_index(path, compression_type):\n",
    "    sisize_of_fileze = 0\n",
    "    term_dictionary_with_positional_gap = preprocess_storing()\n",
    "    \n",
    "    if compression_type == 'no-compression':\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(term_dictionary_with_positional_gap, outfile)\n",
    "       \n",
    "    elif compression_type == 'gamma-code':\n",
    "        term_dictionary_with_positional_gap_gamma_code = prepare_gamma_code(term_dictionary_with_positional_gap)\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(term_dictionary_with_positional_gap_gamma_code, outfile)\n",
    "\n",
    "    elif compression_type == 'variable-byte':\n",
    "        term_dictionary_with_positional_gap_variable_byte = prepare_variable_byte(term_dictionary_with_positional_gap)\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(term_dictionary_with_positional_gap_variable_byte, outfile)\n",
    "    \n",
    "    size_of_file = os.path.getsize(path)\n",
    "    return size_of_file\n",
    "\n",
    "print(\"no compression size : \", store_index(\"no_compression.json\", \"no-compression\"), ' bytes')\n",
    "print(\"gamma code size : \", store_index(\"gamma_code.json\", \"gamma-code\"), ' bytes')\n",
    "print(\"variable byte size : \", store_index(\"variable_byte.json\", \"variable-byte\"), ' bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "همانگونه که مشاهده می‌کنید، در روش gamma code به میزان 25 درصد و در روش variable byte به میزان 35 درصد کاهش سایز داشته‌ایم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompression gamma code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_single_gamma_code(gamma_code):\n",
    "    if gamma_code == 0 and type(gamma_code) is int:\n",
    "        return [0]\n",
    "    \n",
    "    \n",
    "    res_list = []\n",
    "    if type(gamma_code) is float:\n",
    "        res_list.append(1)\n",
    "        gamma_code = int(gamma_code)\n",
    "    gamma_code = decimal_to_binary(gamma_code)\n",
    "    \n",
    "    length = 0\n",
    "    i = 0\n",
    "    state = 0                        # 0: counting, 1: calculating\n",
    "    while i < len(gamma_code):\n",
    "        bit = gamma_code[i]\n",
    "        if state == 0 and bit == '1':\n",
    "            length += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        if state == 0 and bit == '0':\n",
    "            i += 1\n",
    "            state = 1\n",
    "            continue\n",
    "        if state == 1:\n",
    "            res = gamma_code[i:i+length]\n",
    "            res = int('1' + res, 2)\n",
    "            res_list.append(res)\n",
    "            state = 0\n",
    "            i += length\n",
    "            length = 0\n",
    "    return res_list\n",
    "    \n",
    "def gamma_decompression(data):\n",
    "    new_data = dict()\n",
    "    for term, postings in data.items():\n",
    "        new_data[term] = [[] for _ in range(2)]\n",
    "        for i in range(len(postings)):    # i = 0 or 1\n",
    "            posting = postings[i]\n",
    "            posting = posting.split(',')\n",
    "            new_posting = new_data[term][i]\n",
    "            for j in range(int(len(posting) / 2)):\n",
    "                docID = int(posting[2*j])\n",
    "                gamma_code = posting[2*j + 1]\n",
    "                if '.' in gamma_code:\n",
    "                    gamma_code = float(gamma_code)\n",
    "                else:\n",
    "                    gamma_code = int(gamma_code)\n",
    "                positional_list_gap = decompress_single_gamma_code(gamma_code)\n",
    "                new_posting.append([docID, positional_list_gap])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompression Variable Byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_single_variable_byte(variable_byte):\n",
    "    res_list = []\n",
    "    variable_byte = decimal_to_binary(variable_byte)\n",
    "    \n",
    "    if len(variable_byte) % 8 != 0:\n",
    "        variable_byte = '0' * (8 - len(variable_byte) % 8) + variable_byte\n",
    "    i = 0\n",
    "    res = ''\n",
    "    while i < len(variable_byte):\n",
    "        bit = variable_byte[i]\n",
    "        if bit == '0':\n",
    "            res += variable_byte[i+1:i+8]\n",
    "            i += 8\n",
    "            continue\n",
    "        if bit == '1':\n",
    "            res += variable_byte[i+1:i+8]\n",
    "            res_list.append(int(res, 2))\n",
    "            res = ''\n",
    "            i += 8\n",
    "            continue\n",
    "        \n",
    "    return res_list\n",
    "    \n",
    "def variable_byte_decompression(data):\n",
    "    new_data = dict()\n",
    "    for term, postings in data.items():\n",
    "        new_data[term] = [[] for _ in range(2)]\n",
    "        for i in range(len(postings)):    # i = 0 or 1\n",
    "            posting = postings[i]\n",
    "            posting = posting.split(',')\n",
    "            new_posting = new_data[term][i]\n",
    "            for j in range(int(len(posting) / 2)):\n",
    "                docID = int(posting[2*j])\n",
    "                variable_byte = int(posting[2*j+1])\n",
    "                positional_list_gap = decompress_single_variable_byte(variable_byte)\n",
    "                new_posting.append([docID, positional_list_gap])\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_gaps(index):\n",
    "    new_index = dict()\n",
    "\n",
    "    for key, value in index.items():\n",
    "        new_index[key] = [[] for _ in range(2)]\n",
    "        new_list_1 = new_index[key][0]\n",
    "        new_list_2 = new_index[key][1]\n",
    "\n",
    "        old_list_1 = value[0]\n",
    "        old_list_2 = value[1]\n",
    "\n",
    "        for dId_index in range(len(old_list_1)):\n",
    "            dId = old_list_1[dId_index][0]            # dId = document ID\n",
    "            L = old_list_1[dId_index][1]              # positinal index of dId\n",
    "            new_poitional_list = []\n",
    "            for i in range(len(L)):\n",
    "                if i == 0:\n",
    "                    new_poitional_list.append(L[i])\n",
    "                else:\n",
    "                    new_poitional_list.append(L[i] + new_poitional_list[i-1])\n",
    "            new_list_1.append([int(dId), new_poitional_list])\n",
    "\n",
    "\n",
    "        for dId_index in range(len(old_list_2)):\n",
    "            dId = old_list_2[dId_index][0]            # dId = document ID\n",
    "            L = old_list_2[dId_index][1]              # positinal index of dId\n",
    "            new_poitional_list = []\n",
    "            for i in range(len(L)):\n",
    "                if i == 0:\n",
    "                    new_poitional_list.append(L[i])\n",
    "                else:\n",
    "                    new_poitional_list.append(L[i] + new_poitional_list[i-1])\n",
    "            new_list_2.append([int(dId), new_poitional_list])\n",
    "    return new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index(path, compression_type):\n",
    "    index = dict()\n",
    "    \n",
    "    if compression_type == 'no-compression':\n",
    "        with open(path) as outfile:\n",
    "            index = json.load(outfile)\n",
    "\n",
    "    elif compression_type == 'gamma-code':\n",
    "        with open(path) as outfile:\n",
    "            index = json.load(outfile)\n",
    "        index = gamma_decompression(index)\n",
    "\n",
    "    elif compression_type == 'variable-byte':\n",
    "        with open(path) as outfile:\n",
    "            index = json.load(outfile)\n",
    "        index = variable_byte_decompression(index) \n",
    "    \n",
    "    index = convert_gaps(index)\n",
    "    return index\n",
    "\n",
    "\n",
    "retrieved_no_compression = load_index(\"no_compression.json\", \"no-compression\")\n",
    "retrieved_variable_byte = load_index(\"variable_byte.json\", \"variable-byte\")\n",
    "retrieved_gamma_code = load_index(\"gamma_code.json\", \"gamma-code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در تابع load_index ابتدا با توجه به نوع ذخیره‌سازی، فایل مربوطه را load کرده و سپس آن را decompress می‌کنیم. پس از آن با استفاده از تابع convert_gaps، شناسه‌های ذخیره شده postitional به صورت gap را به شناسه‌های عادی تغییر می‌دهیم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i-iLvu2nh2k"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>اصلاح پرسمان (۱۵ نمره امتیازی)</b>\n",
    "    </h1>\n",
    "    در صورتی که پرسمان ورودی دارای غلط املایی باشد یا به عبارتی لغاتی از آن در لغت‌نامه موجود نباشد، لازم است که با جستجوی لغت‌های احتمالی و انتخاب بهترین لغت به ادامه‌ی جستجو با پرسمان اصلاح شده پرداخته شود. برای اینکار ابتدا باید با روش bigram و معیار jaccard نزدیک‌ترین لغات به لغت با غلط املایی را پیدا کنید. سپس با استفاده از معیار edit distance بهترین لغت را از میان آن‌ها بیابید.<br>\n",
    "    نیازی به ذخیره‌سازی و فشرده‌سازی نمایه بایگرم نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams():\n",
    "    bigrams = dict()\n",
    "    for i in range(ord('a'), ord('z') + 1):\n",
    "        char = chr(i)\n",
    "        bigrams['$' + char] = set()\n",
    "        bigrams[char + '$'] = set()\n",
    "        for j in range(ord('a'), ord('z') + 1):\n",
    "            bigrams[char + chr(j)] = set()\n",
    "    return bigrams\n",
    "\n",
    "def create_bigrams_from_terms():\n",
    "    bigrams = create_bigrams()\n",
    "    terms = term_dictionary.keys()\n",
    "    for term in terms:\n",
    "        term = '$' + term + '$'\n",
    "        for i in range(1, len(term)):\n",
    "            try:                       \n",
    "                bigrams[term[i-1:i+1]].add(term[1:-1])\n",
    "            except:\n",
    "                continue\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_by_bigram(term, bigrams):\n",
    "    term = '$' + term + '$'\n",
    "    length = len(term) - 1\n",
    "    first_candidates = dict()\n",
    "    for i in range(1, len(term)):\n",
    "        bigram = term[i-1:i+1]\n",
    "        for c_term in bigrams[bigram]:\n",
    "            try:\n",
    "                first_candidates[c_term] += 1\n",
    "            except:\n",
    "                first_candidates[c_term] = 1\n",
    "    second_candidates = []\n",
    "    for key, value in first_candidates.items():\n",
    "        if ((value)/(len(key) + 1 + length - value)) >= 0.5:\n",
    "            second_candidates.append(key)\n",
    "    return second_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### reference : geeksforgeeks\n",
    "\n",
    "def editDistDP(str1, str2):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)]\n",
    " \n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1): \n",
    "            if i == 0:\n",
    "                dp[i][j] = j  \n",
    "            elif j == 0:\n",
    "                dp[i][j] = i   \n",
    "            elif str1[i-1] == str2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    " \n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1])   \n",
    " \n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequency(term):\n",
    "    posting_1 = term_dictionary[term][0]\n",
    "    posting_2 = term_dictionary[term][1]\n",
    "    return len(posting_1) + len(posting_2)\n",
    "    \n",
    "\n",
    "def get_corrected_term(term):\n",
    "    bigrams = create_bigrams_from_terms()\n",
    "    candidate_terms = get_candidates_by_bigram(term, bigrams)\n",
    "    if len(candidate_terms) == 0:\n",
    "        return ''\n",
    "    candidate_terms_edit_distancies = []\n",
    "    for candidate_term in candidate_terms:\n",
    "        candidate_terms_edit_distancies.append(editDistDP(term, candidate_term) \n",
    "                                               + (1 / get_term_frequency(candidate_term)))  # adding term frequency to make a distinction between similar candidate terms which has same edit distance\n",
    "        \n",
    "    return candidate_terms[candidate_terms_edit_distancies.index(min(candidate_terms_edit_distancies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qMZTstsbL1G3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': ['hello', 'hell', 'he', 'her'], 'el': ['telephone', 'else']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_bigram_index():\n",
    "    \"\"\"Creates the bigram index for spell correction\"\"\"\n",
    "\n",
    "    # TODO: create bigram index\n",
    "    bigram = {\"he\": [\"hello\", \"hell\", \"he\", \"her\"], \"el\": [\"telephone\", \"else\"]}\n",
    "    return bigram\n",
    "\n",
    "create_bigram_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sL3E8C-nL1yG",
    "outputId": "d4da236e-728c-4419-e80e-e8a36bd4e85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the adventur of sherlock holmes '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corrected_text(raw_text):\n",
    "    \"\"\"Corrects the query\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_text : str\n",
    "        Input text that could be a title or a plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The corrected text\n",
    "    \"\"\"\n",
    "    splitted_raw_text = prepare_text(raw_text, False)\n",
    "    tokenized_text = prepare_text(raw_text)\n",
    "    corrected_text = \"\"\n",
    "    for i in range(len(tokenized_text)):\n",
    "        lemmatized_term = tokenized_text[i]\n",
    "        raw_term = splitted_raw_text[i]\n",
    "        if raw_term in stop_words:\n",
    "            corrected_text += raw_term + \" \"\n",
    "            continue\n",
    "        if lemmatized_term in term_dictionary.keys():\n",
    "            corrected_text += raw_term + \" \"\n",
    "        else:\n",
    "            corrected_text += get_corrected_term(lemmatized_term) + \" \"\n",
    "    return corrected_text\n",
    "\n",
    "get_corrected_text(\"the adevntures of herlock holmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl which thyroid cancer meet a boy and fall in love with  final the girl dyes '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_corrected_text('Girl whith thiroid Caner meet a boy and fall in love with himm. finaly the girl dyes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    " برای راحتی کار، از تابع create_bigram_index استفاده نشده است. صرفا صدا زدن تابع get_corrected_text، کوئری داده شده را، در صورتی که مشکلی داشته باشد، اصلاح می‌کند. البته این اصلاح خیلی دقیق نیست ولی برای عملیات جستجو کار‌راه‌انداز می‌باشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BWhdHJbs1zy"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو و بازیابی اسناد (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش جستجو ترتیب‌دار در فضای برداری tf-idf به روش <b>lnn-ltn</b> انجام می‌شود. یک پرسمان title و یک پرسمان plot ورودی گرفته شده و هر کدام در بخش مربوطه از اسناد جستجو می‌شوند. امتیاز نهایی هر سند برابر با جمع وزن‌دار امتیاز به دست آمده از جستجو در بخش title و plot است. به این صورت که وزن plot واحد در نظر گرفته شده و وزن تایتل به عنوان ورودی داده می‌شود.<br>\n",
    "    در نهایت اسناد برتر را نمایش دهید. تعداد حداکثر اسناد برتر نیز به عنوان ورودی داده می‌شود.<br><br>\n",
    "    نکته بسیار مهم نحوه نمایش اسناد انتخابی است. برای نمایش هر سند علاوه بر استفاده از شناسه و تیتر، یک هایلایت برای آن درست کنید. به این معنا که کلمات موجود در پرسمان را که باعث انتخاب سند شده‌اند به همراه ۲-۳ ترم قبل و بعد از آن به عنوان هایلایت آن سند نمایش دهید. اینگونه کاربر می‌تواند خیلی سریع دلیل بازیابی اسناد توسط سامانه را متوجه شود. مشابه کاری که سرچ گوگل انجام می‌دهد و ۲-۳ خط مربوطه را زیر وبسایت‌های پیشنهادی نمایش می‌دهد. طبیعتا راه حل بهینه برای این‌کار استفاده از قابلیت‌های نمایه جایگاهی می‌باشد.<br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت، کدهای مربوط به بخش بازیابی اسناد به صورت رنک بندی شده، زده شده است و عملکرد این قسمت دقیقا مانند عملکرد الگوریتم معرفی شده در اسلاید می‌باشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vector score for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_title(title):\n",
    "    if title == '':\n",
    "        return []\n",
    "    # title\n",
    "    query = title\n",
    "    query = prepare_text(query)\n",
    "    query_len = len(query)\n",
    "    query_dictionary = set(query)\n",
    "    query_dictionary_by_term_freq = dict((_, query.count(_)) for _ in query_dictionary)\n",
    "\n",
    "    ### doc section\n",
    "    query_terms_postings = dict()\n",
    "    for term in query_dictionary:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        posting = term_dictionary[term][0]\n",
    "        term_doc_freq = []\n",
    "        for i in range(len(posting)):\n",
    "            term_doc_freq.append((posting[i][0], 1 + math.log(len(posting[i][1]), 10)))\n",
    "        query_terms_postings[term] = term_doc_freq\n",
    "\n",
    "    doc_vectors = dict()\n",
    "    for term in query_dictionary:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        for i, j in query_terms_postings[term]:\n",
    "            if i in doc_vectors.keys():\n",
    "                pass\n",
    "            else:\n",
    "                doc_vectors[i] = dict((_,0) for _ in query_dictionary)\n",
    "            doc_vectors[i][term] += j\n",
    "\n",
    "    ### query section\n",
    "    # calcualting weights for query\n",
    "    query_vector = dict()\n",
    "    for term in query_dictionary:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        term_tf = query_dictionary_by_term_freq[term]\n",
    "        log_tf = (1 + math.log(term_tf, 10))\n",
    "        N = len(df)             # number of docs\n",
    "        term_df = len(term_dictionary[term][0])\n",
    "        try:\n",
    "            idf = math.log(N / term_df, 10)\n",
    "        except:\n",
    "            idf = 0\n",
    "            \n",
    "        query_vector[term] = log_tf * idf\n",
    "\n",
    "    # normalizing weights for query \n",
    "    # query_vector = {k: v / total for total in (sum(query_vector.values()),) for k, v in query_vector.items()}\n",
    "\n",
    "\n",
    "\n",
    "    ### computing result\n",
    "    doc_scores_title = []\n",
    "    for docID, doc_vector in doc_vectors.items():\n",
    "        score_dict = {k:doc_vector[k]*query_vector[k] for k in query_vector}\n",
    "        score = sum(score_dict.values())\n",
    "        doc_scores_title.append([docID, score, max(score_dict.items(), key=operator.itemgetter(1))[0]])\n",
    "    \n",
    "    return doc_scores_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vector score for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_plot(plot):\n",
    "    # plots\n",
    "    query = plot\n",
    "    query = prepare_text(query)\n",
    "    query_len = len(query)\n",
    "    query_dictionary = set(query)\n",
    "    query_dictionary_by_term_freq = dict((_, query.count(_)) for _ in query_dictionary)\n",
    "\n",
    "    ### doc section\n",
    "    query_terms_postings = dict()\n",
    "    for term in query_dictionary:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        posting = term_dictionary[term][1]\n",
    "        term_doc_freq = []\n",
    "        for i in range(len(posting)):\n",
    "            term_doc_freq.append((posting[i][0], 1 + math.log(len(posting[i][1]), 10)))\n",
    "        query_terms_postings[term] = term_doc_freq\n",
    "\n",
    "    doc_vectors = dict()\n",
    "    for term in query_dictionary:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        for i, j in query_terms_postings[term]:\n",
    "            if i in doc_vectors.keys():\n",
    "                pass\n",
    "            else:\n",
    "                doc_vectors[i] = dict((_,0) for _ in query_dictionary)\n",
    "            doc_vectors[i][term] += j\n",
    "\n",
    "    ### query section\n",
    "    # calcualting weights for query\n",
    "    query_vector = dict()\n",
    "    best_query_term = None\n",
    "    best_query_term_weight = 0\n",
    "    for term in query_dictionary:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        term_tf = query_dictionary_by_term_freq[term]\n",
    "        log_tf = (1 + math.log(term_tf, 10))\n",
    "        N = len(df)             # number of docs\n",
    "        term_df = len(term_dictionary[term][1])\n",
    "        idf = math.log(N / term_df, 10)\n",
    "\n",
    "        query_vector[term] = log_tf * idf\n",
    "\n",
    "    # normalizing weights for query \n",
    "    # query_vector = {k: v / total for total in (sum(query_vector.values()),) for k, v in query_vector.items()}\n",
    "\n",
    "\n",
    "\n",
    "    ### computing result\n",
    "    doc_scores_plot = []\n",
    "    for docID, doc_vector in doc_vectors.items():\n",
    "        score_dict = {k:doc_vector[k]*query_vector[k] for k in query_vector}\n",
    "        score = sum(score_dict.values())\n",
    "        doc_scores_plot.append([docID, score, max(score_dict.items(), key=operator.itemgetter(1))[0]])\n",
    "    \n",
    "    return doc_scores_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_res(doc_scores_title, doc_scores_plot, coef, K):\n",
    "    final_scores = dict([doc_scores_plot[i][0], doc_scores_plot[i]] for i in range(len(doc_scores_plot)))\n",
    "\n",
    "    for i in range(len(doc_scores_title)):\n",
    "        docID = doc_scores_title[i][0]\n",
    "        title_score = doc_scores_title[i][1]\n",
    "        title_word = doc_scores_title[i][2]\n",
    "        if docID not in final_scores.keys():\n",
    "            final_scores[docID] = [docID, title_score, '', title_word]\n",
    "        else:\n",
    "            final_scores[docID][1] = final_scores[docID][1] + coef * title_score \n",
    "            final_scores[docID].append(title_word)\n",
    "\n",
    "    res = list((dict(sorted(final_scores.items(), key=lambda item: item[1][1]))).values())[::-1][:K]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snippet(term_position, docID):\n",
    "    plot = prepare_text(df[df['id'] == docID].iloc[0].at['plot'], False)\n",
    "    res = \"... \"\n",
    "    start = term_position - 3\n",
    "    end = term_position + 3\n",
    "    try:\n",
    "        for i in range(start, end + 1):\n",
    "            if i == term_position:\n",
    "                res += \"<b>\" + plot[i] + \"</b> \"\n",
    "                continue\n",
    "            res += plot[i] + \" \"\n",
    "    except:\n",
    "        pass\n",
    "    res += \" ...\"\n",
    "    return res\n",
    "    \n",
    "def get_snippet(final_res, title_query, plot_query):\n",
    "    new_res = []\n",
    "    \n",
    "    # creats snippet for plot and finds the best term in the title\n",
    "    for res in final_res:\n",
    "        docID = res[0]\n",
    "        \n",
    "        if plot_query != '':\n",
    "            plot_term = res[2]\n",
    "            posting = term_dictionary[plot_term][1]\n",
    "\n",
    "            positional_index = []\n",
    "            for i in range(len(posting)):\n",
    "                if posting[i][0] == docID:\n",
    "                    positional_index = posting[i][1]\n",
    "                    break\n",
    "                    \n",
    "            term_position = positional_index[0]\n",
    "            snippent = create_snippet(term_position, docID)           \n",
    "        else:\n",
    "            snippent = df[df['id'] == docID].iloc[0].at['plot'][:6] + '...'\n",
    "        \n",
    "        \n",
    "        title = df[df['id'] == docID].iloc[0].at['title']\n",
    "        if title_query == '':\n",
    "            new_res.append([docID, title, snippent])\n",
    "            continue\n",
    "        \n",
    "        if len(res) == 4 :            \n",
    "            title_term = res[3]\n",
    "            title_posting_dict = {term_dictionary[title_term][0][j][0]:term_dictionary[title_term][0][j][1] for j in range(len(term_dictionary[title_term][0]))}\n",
    "            title_term_position = title_posting_dict[docID][0]\n",
    "            title_splitted = title.split()\n",
    "            new_title = \"\"\n",
    "            for i in range(len(title_splitted)):\n",
    "                if i != title_term_position:\n",
    "                    new_title += title_splitted[i] + \" \"\n",
    "                    continue\n",
    "                new_title += '<b>' + title_splitted[i] + '</b>' + \" \"\n",
    "            new_res.append([docID, new_title, snippent])\n",
    "        \n",
    "        else:\n",
    "            new_res.append([docID, title, snippent])\n",
    "\n",
    "        \n",
    "    return new_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0R2i_O4Gev_",
    "outputId": "7f57be29-eec0-489e-dd61-74d67c923106"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4,\n",
       "  'The Call of the <b>Wild</b> ',\n",
       "  '... A white girl <b>Florence</b> Lawrence rejects a  ...'],\n",
       " [4864,\n",
       "  '<b>Wild</b> Things ',\n",
       "  '... be good A <b>popular</b> Miami area high  ...'],\n",
       " [2238,\n",
       "  'Davy Crockett, King of the <b>Wild</b> Frontier ',\n",
       "  '... Crockett becomes a <b>popular</b> member of the  ...']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(title_query, plot_query, title_weight, max_result_count=10):\n",
    "    title_query = get_corrected_text(title_query)\n",
    "    plot_query = get_corrected_text(plot_query)\n",
    "    doc_scores_title = compute_score_title(title_query)\n",
    "    doc_scores_plot = compute_score_plot(plot_query)\n",
    "    final_res = compute_final_res(doc_scores_title, doc_scores_plot, title_weight, max_result_count)\n",
    "    final_res = get_snippet(final_res, title_query, plot_query)\n",
    "    return final_res\n",
    "\n",
    "search(\"wild\", \"Florence popular\", 10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBig369C6wSC"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ارزیابی عملکرد سامانه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    همانطور که در بخش مربوط به مجموعه دادگان گفته شد، تعدادی پرسمان نمونه به همراه اسناد مورد نظر برای آن‌ها در اختیار شما قرار گرفته است. در هر مورد اطلاعات لازم برای ایجاد یک پرسمان ذکر شده است. مطابق آن‌ها پرسمان خود را ایجاد کنید. نتایج به دست آمده از هر پرسمان را به عنوان predicted results آن پرسمان در نظر بگیرید. همچنین در هر مورد لیستی از شناسه‌ها وجود دارد. این لیست را به عنوان actual results در نظر بگیرید. <br>\n",
    "    معیار‌های زیر را پیاده‌سازی کنید (بدون استفاده از کد آماده) و نتیجه این معیارها را روی مجموعه‌ی actual و predicted گزارش کنید. دقت کنید که به ازای هر پرسمان باید تمام معیارها را در قالب یک جدول گزارش کنید.<br> دقت کنید که ۳ پرسمان آخر فقط برای افرادی که بخش spell correction را پیاده کرد‌ه‌اند نتیجه مطلوب دارد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPfEVezQIsmb",
    "outputId": "28b08705-5d4f-47e6-baec-3372f3e3293c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(actual, predicted):\n",
    "    intersection_set = set.intersection(set(actual), set(predicted))\n",
    "    intersection_len = len(intersection_set)\n",
    "    return round(intersection_len / len(predicted), 2)\n",
    "\n",
    "precision([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is4nfSw1LQoQ",
    "outputId": "cc2a4388-464b-4d67-cb86-32617e3e526d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall(actual, predicted):\n",
    "    intersection_set = set.intersection(set(actual), set(predicted))\n",
    "    intersection_len = len(intersection_set)\n",
    "    return round(intersection_len / len(actual), 2)\n",
    "    \n",
    "recall([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYHYwRgTLVRA",
    "outputId": "d749daf0-102d-4855-e292-21dcd137d6be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_score(actual, predicted):\n",
    "    precision_number = precision(actual, predicted)\n",
    "    recall_number = recall(actual, predicted)\n",
    "    return(round(2*((precision_number*recall_number)/(precision_number+recall_number)), 2))\n",
    "\n",
    "f1_score([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZg-_PUPLZ31",
    "outputId": "e85b9efa-3b9b-470e-8b93-f474585da6c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_score(actual, predicted):\n",
    "    res = 0\n",
    "    numbers = 0\n",
    "    for i in range(len(predicted)):\n",
    "        docID = predicted[i]\n",
    "        if docID in actual:\n",
    "            numbers += 1\n",
    "            res += (numbers) / (i + 1)\n",
    "    return (round(res/len(actual), 2))\n",
    "\n",
    "map_score([1,2], [1, 4, 5])\n",
    "# map_score([1, 2, 3, 4, 5], [2, 6, 1, 7, 9, 3, 10, 8, 5, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "برای پیاده‌سازی ndcg_score، امتیاز هر داک موجود در لیست predicted را در صورتی که در داخل لیست acutal نیز قرار داشته باشد، 1 در نظر گرفته و درغیراین‎صورت، 0 در نظر گرفته‌شده‌است. همچنین فرمول محاسبه DCG   بر اساس فرمول زیر پیاده‌سازی شده است:\n",
    "$$ DCG_p = rel_1 + \\sum_{i=2}^{p} \\frac{rel_i}{log_{2} i} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgOKEbMNLf-r",
    "outputId": "b38f683a-7224-411b-efb4-8fe7ce4eb328"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266979222386447"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_dcg(relevency_list):\n",
    "    res_list = []\n",
    "    for i in range(len(relevency_list)):\n",
    "        if i == 0:\n",
    "            res = relevency_list[i]\n",
    "        else:\n",
    "            res = ((relevency_list[i]) / math.log2(i+1)) + res_list[i-1]\n",
    "        res_list.append(res)\n",
    "    \n",
    "    return res_list\n",
    "\n",
    "def ndcg_score(actual, predicted):\n",
    "    relevency_list = []\n",
    "    for i in predicted:\n",
    "        if i in actual:\n",
    "            relevency_list.append(1)\n",
    "        else:\n",
    "            relevency_list.append(0)\n",
    "    actual_dcg_list = calculate_dcg(relevency_list)\n",
    "    ideal_dcg_list = calculate_dcg([1] * len(actual))\n",
    "    \n",
    "    for i in range(len(actual_dcg_list)):\n",
    "        actual_dcg_list[i] /= ideal_dcg_list[i]\n",
    "    \n",
    "    return sum(actual_dcg_list) / len(actual_dcg_list)\n",
    "\n",
    "ndcg_score([1, 2, 7], [1, 4, 5])   # actual and predicted list must have equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation.json', 'r') as outfile:\n",
    "    validation_list = json.load(outfile)\n",
    "validation_dic = {'query' + str(i) : validation_list[i] for i in range(len(validation_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_lists = dict()\n",
    "predicted_lists = dict()\n",
    "\n",
    "for key, value in validation_dic.items():\n",
    "    actual_lists[key] = value['doc_ids']\n",
    "    plot_query = value['plot_query']\n",
    "    title_query = value['title_query']\n",
    "    max_size = value['max_size']\n",
    "    title_weight = value['title_weight']\n",
    "    \n",
    "    search_result = search(title_query, plot_query, title_weight, max_size)\n",
    "    predicted_list = [search_result[i][0] for i in range(len(search_result))]\n",
    "    predicted_lists[key] = predicted_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['query', 'precision', 'recall', 'f1_score', 'map_score', 'ndcg_score'])\n",
    "for key, value in actual_lists.items():\n",
    "    new_res = dict()\n",
    "    \n",
    "    new_res['query'] = key\n",
    "    new_res['precision'] = precision(actual_lists[key], predicted_lists[key])\n",
    "    new_res['recall'] = recall(actual_lists[key], predicted_lists[key])\n",
    "    new_res['f1_score'] = f1_score(actual_lists[key], predicted_lists[key])\n",
    "    new_res['map_score'] = map_score(actual_lists[key], predicted_lists[key])\n",
    "    new_res['ndcg_score'] = ndcg_score(actual_lists[key], predicted_lists[key])\n",
    "    \n",
    "    result_df = result_df.append(new_res, ignore_index=True)\n",
    "    \n",
    "result_df.set_index('query', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>map_score</th>\n",
       "      <th>ndcg_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>query0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query3</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.796796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision  recall  f1_score  map_score  ndcg_score\n",
       "query                                                     \n",
       "query0       1.00    1.00      1.00       1.00    1.000000\n",
       "query1       1.00    1.00      1.00       1.00    1.000000\n",
       "query2       1.00    1.00      1.00       1.00    1.000000\n",
       "query3       0.67    0.67      0.67       0.53    0.796796\n",
       "query4       1.00    1.00      1.00       1.00    1.000000\n",
       "query5       1.00    1.00      1.00       1.00    1.000000\n",
       "query6       1.00    1.00      1.00       1.00    1.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHVlw0kVBUW4"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نکات پایانی</b>\n",
    "    </h1>\n",
    "    \n",
    "\n",
    "1.   سیستم را بهینه پیاده‌سازی کنید تا در زمان کمتری بارگذاری و نمایه‌سازی  انجام شود.\n",
    "2.   تمام قطعه‌ کدهای بالا باید توسط شما تکمیل شود. نوت‌بوک نهایی باید بدون خطا اجرا شود. اگر تمام کدهای استفاده شده در همین فایل نیست،‌ حتما آن‌ها را نیز به همراه نوت‌بوک در کوئرا آپلود کنید.\n",
    "3.    اسم توابع و نحوه ورودی گرفتن و خروجی دادن آن‌ها را تغییر ندهید. بقیه اجزای توابع صرفا برای شهود بیشتر شما نوشته شده‌اند و نیازی به نگه‌داری آن‌ها نیست.\n",
    "4.   در صورت امکان استفاده از کد آماده مشخصا این مورد برای بخش مربوطه ذکر شده است. اگر چیزی در این مورد ذکر نشده نمی‌توانید از کد آماده استفاده کنید.\n",
    "5.    فایل داده‌ها را در کوئرا آپلود نکنید.\n",
    "6.    فایل زیپ نهایی و فایل نوت‌بوک حتما به فرمت StudentNumber_phase1 نامگذاری شود.\n",
    "\n",
    "\n",
    "<b>سالم و موفق باشید.</b>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIR_Phase1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
