{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jsonlines\n",
    "# !pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import jsonlines\n",
    "import typer\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "from scipy.linalg import svd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_path = 'CrawledPapers.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "    driver = webdriver.Firefox()\n",
    "\n",
    "    infos = dict()\n",
    "    ids = set()\n",
    "\n",
    "    # try:\n",
    "    #     with jsonlines.open('CrawledPapers.json') as reader:\n",
    "    #         for obj in reader:\n",
    "    #             ids.add(obj['id'])\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    List = []\n",
    "\n",
    "    i = 0\n",
    "    json_length = 0\n",
    "    while json_length < 2000:\n",
    "        try:\n",
    "            info = dict()\n",
    "\n",
    "            file1 = open('start.txt', 'r')\n",
    "            Lines = file1.readlines()\n",
    "            file1.close()\n",
    "            url = Lines[i]\n",
    "\n",
    "            # extract and check id\n",
    "            id = re.search(r'^(https://)?(academic\\.microsoft\\.com/)?(paper/)?(\\d+)', url).group(4)\n",
    "            if id in ids:\n",
    "                i += 1\n",
    "                continue\n",
    "            info['id'] = id\n",
    "            ids.add(id)\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            time.sleep(4)\n",
    "\n",
    "            # extract name\n",
    "            name = driver.find_element_by_css_selector(\"h1.name\").text\n",
    "            info['name'] = name\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # extract abstract\n",
    "            abstract = driver.find_element_by_xpath(\"//div[@class='name-section']/p\").text\n",
    "            info['abstract'] = abstract\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # extract date\n",
    "            date = driver.find_elements_by_xpath(\"//div[@class='name-section']/a[@data-appinsights-action='OpenVenueDetails']/span[@class='year']\")[0].text\n",
    "            info['date'] = date\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # extract authors\n",
    "            author_list = driver.find_elements_by_xpath(\"//div[@class='authors']\")[0].text\n",
    "            author_list = ''.join(i for i in author_list if not i.isdigit())\n",
    "            author_list = author_list.strip()\n",
    "            author_list = author_list.split(\" , \")\n",
    "            info['authors'] = author_list\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # extract related_topic\n",
    "            topic = driver.find_elements_by_xpath(\"//div[@class='text au-target']\")\n",
    "            topics = []\n",
    "            for k in range(3):\n",
    "                try:\n",
    "                    topics.append(topic[k].text)\n",
    "                except:\n",
    "                    pass\n",
    "            info['related topic'] = topics\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # extract citation count and reference count\n",
    "            re_ci = driver.find_elements_by_xpath(\"//div[@class='ma-statistics-item au-target']/div[@class='data']/div[@class='count']\")\n",
    "            reference_count = re_ci[0].text\n",
    "            citation_count = re_ci[1].text\n",
    "            info['reference_count'] = reference_count\n",
    "            info['citation_count'] = citation_count\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            ##### driver.find_elements_by_xpath(\"//div[@class='routes-group']/ma-call-to-action\")[1].click()\n",
    "\n",
    "\n",
    "            # extract references\n",
    "            results_section = driver.find_element_by_xpath(\"//div[@class='results']\")\n",
    "            papers = results_section.find_elements_by_xpath(\"//a[@class='title au-target']\")\n",
    "            links = []\n",
    "            try:\n",
    "                links = [paper.get_attribute('href') for paper in papers]\n",
    "            except:\n",
    "                pass\n",
    "            if len(links) >= 10:\n",
    "                links = links[:10]\n",
    "            referenced_ids = []\n",
    "            if reference_count != 0:\n",
    "                for k in range(len(links)):\n",
    "                    link = links[k]\n",
    "                    link_id = re.search(r'^(https://)?(academic\\.microsoft\\.com/)?(paper/)?(\\d+)', link).group(4)\n",
    "                    if link_id in ids:\n",
    "                        continue\n",
    "                    link = \"https://academic.microsoft.com/paper/\" + str(link_id)\n",
    "                    referenced_ids.append(link_id)\n",
    "                    with open(\"start.txt\", \"a\") as file2:\n",
    "                        file2.write(\"\\n\")\n",
    "                        file2.write(link)\n",
    "            info['references'] = referenced_ids\n",
    "\n",
    "            time.sleep(4)\n",
    "\n",
    "            List.append(info)\n",
    "\n",
    "            i += 1\n",
    "            json_length +=1\n",
    "\n",
    "            if json_length % 50 == 0:\n",
    "                print(json_length, 'papers crawled')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('****************************************************')\n",
    "            continue\n",
    "\n",
    "    with open('CrawledPapers.json', 'w') as file:\n",
    "        json_string = json.dumps(List, default=lambda o: o.__dict__, indent=2)\n",
    "        file.write(json_string)\n",
    "\n",
    "    print(\"crwaling finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "    \n",
    "    def add_new_edge(self, parent_id, child_ids):\n",
    "        parent = self.find_node(parent_id)\n",
    "        for child_id in child_ids:\n",
    "            child = self.find_node(child_id)\n",
    "            parent.add_child(child)\n",
    "            child.add_parent(parent)\n",
    "        \n",
    "    def find_node(self, id):\n",
    "        response, node = self.is_node_in_graph(id)\n",
    "        if response:\n",
    "             return node\n",
    "        else:\n",
    "            new_node = Node(id)\n",
    "            self.nodes.append(new_node)\n",
    "            return new_node\n",
    "    \n",
    "    def is_node_in_graph(self, id):\n",
    "        for node in self.nodes:\n",
    "            if id == node.id:\n",
    "                return True, node\n",
    "        return False, None\n",
    "    \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.page_rank = 1\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "       \n",
    "    def len_children(self):\n",
    "        return len(self.children)\n",
    "    \n",
    "    def len_parents(self):\n",
    "        return len(self.parents)\n",
    "    \n",
    "    def update_page_rank(self, d, n):\n",
    "        up_neighbors = self.parents\n",
    "        sum_ = 0\n",
    "        for node in up_neighbors:\n",
    "            try:\n",
    "                sum_ += node.page_rank/self.len_children()\n",
    "            except:\n",
    "                pass\n",
    "        self.page_rank = (d/n) + (1-d)*sum_\n",
    "    \n",
    "    def add_child(self, new_child):\n",
    "        for child in self.children:\n",
    "            if child.id == new_child.id:\n",
    "                return\n",
    "        self.children.append(new_child)\n",
    "        \n",
    "    def add_parent(self, new_parent):\n",
    "        for parent in self.parents:\n",
    "            if parent.id == new_parent.id:\n",
    "                return\n",
    "        self.parents.append(new_parent)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_page_ranks(graph, d=0.15, iteration=100):\n",
    "    for i in range(iteration):\n",
    "        nodes = graph.nodes\n",
    "        n = len(nodes)\n",
    "        for node in nodes:\n",
    "            node.update_page_rank(d, n)\n",
    "\n",
    "    page_ranks = dict()\n",
    "    for node in graph.nodes:\n",
    "        page_ranks[node.id] = node.page_rank\n",
    "\n",
    "    with open('PageRank.json', 'w') as fp:\n",
    "            json.dump(page_ranks, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank():\n",
    "    graph = Graph()\n",
    "\n",
    "    with open(crawled_path) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        for obj in json_data:\n",
    "            parent_id = obj['id']\n",
    "            child_ids = obj['references']\n",
    "            graph.add_new_edge(parent_id, child_ids)\n",
    "\n",
    "    create_page_ranks(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Authors_Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "    \n",
    "    def add_new_edge(self, parent_name, child_names):\n",
    "        parent = self.find_node(parent_name)\n",
    "        for child_name in child_names:\n",
    "            child = self.find_node(child_name)\n",
    "            parent.add_child(child)\n",
    "            child.add_parent(parent)\n",
    "        \n",
    "    def find_node(self, name):\n",
    "        response, node = self.is_node_in_graph(name)\n",
    "        if response:\n",
    "             return node\n",
    "        else:\n",
    "            new_node = Author_Node(name)\n",
    "            self.nodes.append(new_node)\n",
    "            return new_node\n",
    "    \n",
    "    def is_node_in_graph(self, name):\n",
    "        for node in self.nodes:\n",
    "            if name == node.name:\n",
    "                return True, node\n",
    "        return False, None\n",
    "    \n",
    "    def normalize_hub_auth(self):\n",
    "        sum_hub = sum(node.hub for node in self.nodes)\n",
    "        sum_auth = sum(node.auth for node in self.nodes)\n",
    "        for node in self.nodes:\n",
    "            node.hub = node.hub/sum_hub\n",
    "            node.auth = node.auth/sum_auth\n",
    "        \n",
    "\n",
    "class Author_Node:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.hub = 1\n",
    "        self.auth = 1\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "       \n",
    "    def len_children(self):\n",
    "        return len(self.children)\n",
    "    \n",
    "    def len_parents(self):\n",
    "        return len(self.parents)\n",
    "    \n",
    "    def add_child(self, new_child):\n",
    "        for child in self.children:\n",
    "            if child.name == new_child.name:\n",
    "                return\n",
    "        self.children.append(new_child)\n",
    "        \n",
    "    def add_parent(self, new_parent):\n",
    "        for parent in self.parents:\n",
    "            if parent.name == new_parent.name:\n",
    "                return\n",
    "        self.parents.append(new_parent)\n",
    "    \n",
    "    def update_hub(self):\n",
    "        res = 0\n",
    "        for node in self.children:\n",
    "            res += node.auth\n",
    "        self.hub = res\n",
    "    \n",
    "    def update_auth(self):\n",
    "        res = 0\n",
    "        for node in self.parents:\n",
    "            res += node.hub\n",
    "        self.auth = res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_best_authors(authors_graph, n_best=10, iteration=5):\n",
    "    for i in range(iteration):\n",
    "        nodes = authors_graph.nodes\n",
    "        n = len(nodes)\n",
    "        for node in nodes:\n",
    "            node.update_auth()\n",
    "        for node in nodes:\n",
    "            node.update_hub()\n",
    "        authors_graph.normalize_hub_auth()\n",
    "    authors_authority = dict()\n",
    "    for node in authors_graph.nodes:\n",
    "        authors_authority[node.name] = node.auth\n",
    "    authors_authority = {k: v for k, v in sorted(authors_authority.items(), key=lambda item: item[1])}\n",
    "    return list(authors_authority.items())[-n_best:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HITS_algorithm():\n",
    "    paper_authors = dict()\n",
    "    paper_references = dict()\n",
    "            \n",
    "    with open(crawled_path) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        for obj in json_data:\n",
    "            id = obj['id']\n",
    "            authors = obj['authors']\n",
    "            references = obj['references']\n",
    "            paper_authors[id] = authors\n",
    "            paper_references[id] = references\n",
    "\n",
    "    authors_graph = Authors_Graph()\n",
    "    for id, authors in paper_authors.items():\n",
    "        references = paper_references[id]\n",
    "        for parent_author in authors:\n",
    "            for reference_id in references:\n",
    "                try:\n",
    "                    authors_graph.add_new_edge(parent_author, paper_authors[reference_id])\n",
    "                except:\n",
    "                    pass\n",
    "    return get_n_best_authors(authors_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_related_topics = set()\n",
    "all_related_topics_count = dict()\n",
    "\n",
    "class Paper:\n",
    "    def __init__(self, params):\n",
    "        self.id = params['id']\n",
    "        self.related_topics = params['related topic']    ###\n",
    "        \n",
    "        global all_related_topics\n",
    "        for related_topic in self.related_topics:\n",
    "            all_related_topics.add(related_topic)\n",
    "            try:\n",
    "                all_related_topics_count[related_topic] += 1\n",
    "            except:\n",
    "                all_related_topics_count[related_topic] = 1\n",
    "    \n",
    "    def set_vector(self, vector):\n",
    "        self.vector = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []     \n",
    "json_data = []\n",
    "with open(crawled_path) as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "    for obj in json_data:\n",
    "        paper = Paper(obj)\n",
    "        papers.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_related_topics_count = list(all_related_topics_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_related_topics = list(all_related_topics)\n",
    "all_related_topics = [x.lower() for x in all_related_topics]\n",
    "df_columns = df.columns\n",
    "df_columns = [x.lower() for x in df_columns]\n",
    "df_columns = [x.strip() for x in df_columns]\n",
    "\n",
    "swapping_dict = {\n",
    "    'derivative-free optimization':'algorithms',\n",
    "    'ibm pc compatible':'computer hardware',\n",
    "    'structural classification of proteins database':'ai',\n",
    "    'margin (machine learning)':'ai',\n",
    "    'iterative deepening depth-first search':'algorithms',\n",
    "    'linear hashing':'algorithms',\n",
    "    'recurrent neural network language models':'ai',\n",
    "    'hill climbing':'algorithms',\n",
    "    'computer science':'theoretical computer science',\n",
    "    'artificial intelligence':'ai',\n",
    "    'mathematics':'theoretical computer science',\n",
    "    'biology':'theoretical computer science',\n",
    "    'pattern recognition':'ai',\n",
    "    'machine learning':'ai',\n",
    "    'artificial neural network':'ai',\n",
    "    'algorithm':'algorithms',\n",
    "    'computer vision':'ai',\n",
    "    'speech recognition': 'natural language processing',\n",
    "    'mathematical optimization':'algorithms',\n",
    "    'information retrieval':'natural language processing',\n",
    "    'language model':'natural language processing',\n",
    "    'cognitive neuroscience of visual object recognition':'theoretical computer science',\n",
    "    'probabilistic logic':'theoretical computer science',\n",
    "    'unsupervised learning':'ai',\n",
    "    'recurrent neural network':'ai',\n",
    "    'word (computer architecture)':'computer hardware',\n",
    "    'statistics':'theoretical computer science',\n",
    "    'backpropagation':'ai',\n",
    "    'machine translation':'ai',\n",
    "    'set (abstract data type)':'theoretical computer science',\n",
    "    'deep learning':'ai',\n",
    "    'feature extraction':'ai',\n",
    "    'semi-supervised learning':'ai',\n",
    "    'support vector machine':'ai',\n",
    "    'applied mathematics':'theoretical computer science',\n",
    "    'object detection':'ai',\n",
    "    'cluster analysis':'theoretical computer science',\n",
    "    'image processing':'ai',\n",
    "    'inference':'algorithms',\n",
    "    'pattern recognition (psychology)':'ai',\n",
    "    'parsing':'natural language processing',\n",
    "    'task (project management)':'software engineering',\n",
    "    'context (language use)':'software engineering',\n",
    "    'linguistics':'natural language processing',\n",
    "    'sentence':'natural language processing',\n",
    "    'discrete mathematics':'theoretical computer science',\n",
    "    'segmentation':'natural language processing',\n",
    "    'representation (mathematics)':'theoretical computer science',\n",
    "    'natural language':'natural language processing',\n",
    "    'structure (mathematical logic)':'software engineering',\n",
    "    'training set':'ai',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in papers:\n",
    "    main_topics = []\n",
    "    vector = {x:0 for x in df_columns}\n",
    "    for topic in paper.related_topics:\n",
    "        topic = topic.lower()\n",
    "        if topic in df_columns:\n",
    "            vector[topic] = 1\n",
    "        else:\n",
    "            try:\n",
    "                key = topic\n",
    "                value = swapping_dict[key]\n",
    "                vector[value] = 1\n",
    "            except:\n",
    "                pass\n",
    "        paper.set_vector(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_recommender(user_profile, papers=papers):                 # user_profile is pandas series\n",
    "    user_profile = user_profile.fillna(0)\n",
    "    user_profile = dict(user_profile)\n",
    "    papers_scores = dict()\n",
    "    for paper in papers:\n",
    "        score = np.dot(np.array(list(paper.vector.values())), np.array(list(user_profile.values())))\n",
    "        papers_scores[paper.id] = score\n",
    "    papers_scores = {k: v for k, v in sorted(papers_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return list(papers_scores.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2952632681', 0.6800441014466138),\n",
       " ('78159342', 0.6800441014466138),\n",
       " ('2964153729', 0.6094595951857205),\n",
       " ('2145094598', 0.6094595951857205),\n",
       " ('2147800946', 0.6094595951857205),\n",
       " ('2899771611', 0.44820417038369104),\n",
       " ('2129116707', 0.44820417038369104),\n",
       " ('2795241978', 0.44820417038369104),\n",
       " ('1997011019', 0.44820417038369104),\n",
       " ('3012395598', 0.44820417038369104)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_based_recommender(df.loc[8], papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy = df_copy.fillna(0)\n",
    "\n",
    "def collaborative_filtering_recommender(user_profile_id, N):\n",
    "    global df_copy, papers\n",
    "    user_profile = df_copy.loc[user_profile_id]\n",
    "    other_user_profiles_sim = dict()\n",
    "    \n",
    "    for i in range(len(df_copy)):\n",
    "        if user_profile_id != i:\n",
    "            other_user_profiles_sim[i] = np.dot(np.array(df_copy.loc[i]), user_profile)\n",
    "    \n",
    "    other_user_profiles_sim = {k: v for k, v in sorted(other_user_profiles_sim.items(), key=lambda item: item[1], reverse=True)}\n",
    "    selected_users_id = list(other_user_profiles_sim.keys())[:N]\n",
    "        \n",
    "    new_df = df_copy.loc[selected_users_id]\n",
    "    new_df_mean = new_df.mean()\n",
    "    \n",
    "    columns = user_profile.index\n",
    "    new_user_profile = pd.Series(index=columns, data=np.zeros(len(columns)))\n",
    "    for c in columns:\n",
    "        if user_profile[c] == 0.000000:\n",
    "            new_user_profile[c] = new_df_mean[c]\n",
    "        else:\n",
    "            new_user_profile[c] = user_profile[c]\n",
    "    new_user_profile = new_user_profile / np.sum(new_user_profile)\n",
    "    recomended_papers = content_based_recommender(new_user_profile, papers)\n",
    "    return(new_user_profile, recomended_papers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = df.fillna(0).to_numpy()\n",
    "x = A.nonzero()[0]\n",
    "y = A.nonzero()[1]\n",
    "\n",
    "length = len(x)\n",
    "indices = np.arange(length)\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:int(0.8*length)]\n",
    "test_indices = indices[int(0.8*length):]\n",
    "\n",
    "x_train_indices = x[train_indices]\n",
    "y_train_indices = y[train_indices] \n",
    "\n",
    "x_test_indices = x[test_indices]\n",
    "y_test_indices = y[test_indices]\n",
    "\n",
    "A_train = np.zeros(A.shape)\n",
    "A_test = np.zeros(A.shape)\n",
    "\n",
    "for i in range(len(x_train_indices)):\n",
    "    x = x_train_indices[i]\n",
    "    y = y_train_indices[i]\n",
    "    A_train[x, y] = A[x, y]\n",
    "\n",
    "for i in range(len(x_test_indices)):\n",
    "    x = x_test_indices[i]\n",
    "    y = y_test_indices[i]\n",
    "    A_test[x, y] = A[x, y]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(A, P, Q, k):\n",
    "    n = A.shape[0]\n",
    "    m = A.shape[1]\n",
    "    return np.sum(np.power(A - np.dot(P, Q), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, P, Q, k, lr=0.0001, s=5000):\n",
    "    n, m = A.shape\n",
    "    \n",
    "    # updating P\n",
    "    for t in range(k):\n",
    "        for i in range(n):\n",
    "            P[i, t] -= 2 * lr * np.sum(np.matmul((A[i, :] - np.dot(P[i, :], Q)), Q[t, :]))\n",
    "\n",
    "    # updating Q\n",
    "    for t in range(k):\n",
    "        for j in range(m):\n",
    "            Q[t, j] -= 2 * lr * np.sum(np.matmul((A[:, j] - np.dot(P, Q[:, j])), P[:, t]))\n",
    "            \n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = svd(A_train, full_matrices=False)\n",
    "U = U[:, :2]\n",
    "S = S[:2]\n",
    "V = V[:2, :]\n",
    "\n",
    "P, Q = gradient_descent(A_train, U.copy(), V.copy(), U.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066.953190770168\n",
      "3067.650456421008\n"
     ]
    }
   ],
   "source": [
    "print(calculate_cost(A_train, U, V, 2))\n",
    "print(calculate_cost(A_train, P, Q, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803.9800222672209\n",
      "803.974835751723\n"
     ]
    }
   ],
   "source": [
    "print(calculate_cost(A_test, U, V, 2))\n",
    "print(calculate_cost(A_test, P, Q, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.theme('dark grey 9')\n",
    "\n",
    "layout = [[sg.Button('crawl!'), sg.Button('calculate pagerank'), sg.Button('calculate HITS')],\n",
    "         [sg.Text(\"enter user id:\")],\n",
    "         [sg.InputText(size=(12,)), sg.Button('content based recommender')],\n",
    "         [],\n",
    "         [sg.Text(\"enter user id:\"), sg.Text(\"enter N:\")] ,\n",
    "         [sg.InputText(size=(12,)), sg.InputText(size=(12,)), sg.Button('collabrative filtering recommender')]]\n",
    "\n",
    "event, values = sg.Window('List Comprehensions', layout).read(close=True)\n",
    "\n",
    "if event == 'crawl!':\n",
    "    crawl()\n",
    "    sg.popup(\"Crawling Finished! and Results saved in\", crawled_path)\n",
    "\n",
    "    \n",
    "if event == 'calculate pagerank':\n",
    "    pagerank()\n",
    "    sg.popup(\"PageRanking Algorithm finished and results saved in PageRank.json\")\n",
    "\n",
    "    \n",
    "if event == 'calculate HITS':\n",
    "    best_authors = HITS_algorithm()\n",
    "    authors = [x[0] for x in best_authors][::-1]\n",
    "    scores = [x[1] for x in best_authors][::-1]\n",
    "    layout = []\n",
    "    \n",
    "    sz=(10,20)\n",
    "    col1=[[sg.Frame('Authors',[[sg.Text(x)] for x in authors], background_color='#68768C', size=sz)]]\n",
    "    col2=[[sg.Frame('Authority',[[sg.Text(round(x, 4))] for x in scores], background_color='#2E3E58', size=sz)]]\n",
    "\n",
    "    layout = [[sg.Column(col1, element_justification='c' ), sg.Column(col2, element_justification='c')]]\n",
    "\n",
    "    window =sg.Window(\"HITS\",layout)\n",
    "    event,values=window.read()\n",
    "\n",
    "    window.close()   \n",
    "    \n",
    "    \n",
    "if event == 'content based recommender':\n",
    "    id = int(values[0])\n",
    "    recommends = content_based_recommender(df.loc[id])\n",
    "    \n",
    "    sz=(10,20)\n",
    "    col1=[[sg.Frame('Paper id',[[sg.Text(x[0])] for x in recommends], background_color='#68768C', size=sz)]]\n",
    "    col2=[[sg.Frame('Score',[[sg.Text(round(x[1], 4))] for x in recommends], background_color='#2E3E58', size=sz)]]\n",
    "\n",
    "    layout = [[sg.Column(col1, element_justification='c' ), sg.Column(col2, element_justification='c')]]\n",
    "\n",
    "    window =sg.Window('Recomended Papers',layout)\n",
    "    event,values=window.read()\n",
    "    print(event)\n",
    "\n",
    "    window.close() \n",
    "    \n",
    "    \n",
    "if event == 'collabrative filtering recommender':\n",
    "    id = int(values[1])\n",
    "    N = int(values[2])\n",
    "    new_user_profile, recommends = collaborative_filtering_recommender(id, N)\n",
    "    \n",
    "    new_user_profile = {k:v for k,v in sorted(new_user_profile.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    sz1=(10,30)\n",
    "    col1=[[sg.Frame('Paper id',[[sg.Text(x)] for x in list(new_user_profile.keys())[:20]], background_color='#68768C', size=sz1)]]\n",
    "    col2=[[sg.Frame('Score',[[sg.Text(round(x, 4))] for x in list(new_user_profile.values())[:20]], background_color='#2E3E58', size=sz1)]]\n",
    "\n",
    "    layout = [[sg.Column(col1, element_justification='c' ), sg.Column(col2, element_justification='c')],\n",
    "             [sg.Button(\"Recommends\")]]\n",
    "\n",
    "    window =sg.Window('user new profile and recomended papers',layout)\n",
    "    event,values=window.read(close=True)\n",
    "    \n",
    "    sz=(10,20)\n",
    "    col1=[[sg.Frame('Paper id',[[sg.Text(x[0])] for x in recommends], background_color='#68768C', size=sz)]]\n",
    "    col2=[[sg.Frame('Score',[[sg.Text(round(x[1], 4))] for x in recommends], background_color='#2E3E58', size=sz)]]\n",
    "\n",
    "    layout = [[sg.Column(col1, element_justification='c' ), sg.Column(col2, element_justification='c')]]\n",
    "\n",
    "    window =sg.Window('Recomended Papers',layout)\n",
    "    event,values=window.read()\n",
    "\n",
    "    window.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
